{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QtiSbp17oZb"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('breast_cancer_data2.csv')"
      ],
      "metadata": {
        "id": "epgiNMhn8Km7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "182246ba-6926-4d50-c230-201c21c64381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'breast_cancer_data2.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-76dca369455b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'breast_cancer_data2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'breast_cancer_data2.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ILXIxYNz8KjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "-qxzc7Ib8Kgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "4y9iqHuH8Kd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information about data like Data type, columns name,size of data"
      ],
      "metadata": {
        "id": "L-jryXuV8npg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "6SGcgr1f8Kbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check NULL Values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "s3DASLH18KZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Duplicates\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "4xFSl4AM8KWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Visualization Library\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "1qVccY5J8KUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check ratio of Diagnosis\n",
        "df['diagnosis'].value_counts().plot(kind='bar')\n",
        "plt.xticks(rotation=0)\n",
        "plt.title('Distributiion of Diagnosis')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i6JFsVAP8KR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Histogram Plots for each variable\n",
        "df.hist(bins=20,figsize=(20,15),layout=(3,2))\n",
        "ddplt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KZAz19bI8KPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the Box plot for check the outliers in Data\n",
        "df.plot(kind='box',figsize=(20,15),subplots=True,layout=(3,2))\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "im2PXVKI8KMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between variables\n",
        "data=df.corr()\n",
        "data"
      ],
      "metadata": {
        "id": "6u4Ibb0Z8KJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Heatmap to show correlation for better visualization\n",
        "# Plot heatmap of the correlation matrix\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(data, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oAGb4KhS8KHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pairplot\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.pairplot(df,hue='diagnosis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h839uoV28KFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ML library scikit to make classification models\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "j5dYN3nH8KC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting Dependent and Independent Variables\n",
        "x = df[['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', 'mean_smoothness']]\n",
        "y = df['diagnosis']"
      ],
      "metadata": {
        "id": "UplqZDWK8KAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample of independent variable columns\n",
        "x.sample(5)"
      ],
      "metadata": {
        "id": "gKvBirfu9tlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample of Dependent variable column 'diagnosis'\n",
        "y.sample(5)"
      ],
      "metadata": {
        "id": "pRASvIfy9ths"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the data For Training and Testing Sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "CBxGdpBn9tfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of Training and Testing data\n",
        "x_train.shape , y_train.shape , x_test.shape , y_test.shape"
      ],
      "metadata": {
        "id": "fsHeEMKf9tdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler standardizes features by removing the mean and scaling to unit variance. This means that the transformed feature will have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "The formula for standardization is:\n",
        "\n",
        "𝑧\n",
        "( 𝑥 − 𝜇 )/ 𝜎\n",
        "\n",
        "where:\n",
        "\n",
        "𝑥 x is the original feature value.\n",
        "\n",
        "𝜇 μ is the mean of the feature.\n",
        "\n",
        "𝜎 σ is the standard deviation of the feature."
      ],
      "metadata": {
        "id": "M1UXbdCI_Gj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "12fiMgcu9taz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Model"
      ],
      "metadata": {
        "id": "aN1necoe_MdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = LogisticRegression()\n",
        "Model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "7LnaHDtg-11G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = Model.predict(x_test)"
      ],
      "metadata": {
        "id": "6NAzx-t8-1xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred)*100,'%')"
      ],
      "metadata": {
        "id": "ydcs4sZ8-1vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression after Hypertuning"
      ],
      "metadata": {
        "id": "5fVF-HvUS9eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],    # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],          # Regularization type\n",
        "    'solver': ['liblinear']           # Solver (liblinear supports l1 and l2)\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "KwMZctkYS92t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'best_model' is the Logistic Regression model with the best parameters from the GridSearchCV\n",
        "# and X_train contains the feature names\n",
        "\n",
        "# Retrieve the coefficients from the model\n",
        "coefficients = best_model.coef_[0]  # For binary classification, it's a 1D array\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by the absolute value of coefficients\n",
        "feature_importance_df['Absolute Coefficient'] = np.abs(feature_importance_df['Coefficient'])\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Coefficient'], color='skyblue')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance in Logistic Regression')\n",
        "plt.gca().invert_yaxis()  # Invert the y-axis to have the most important feature on top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p0j5W5yLRBTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming y_test and y_pred have been defined from the previous steps\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm_lr = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "cm_df_lr = pd.DataFrame(cm_lr, index=['Actual Benign', 'Actual Malignant'], columns=['Predicted Benign', 'Predicted Malignant'])\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df_lr, annot=True, fmt='d', cmap='Blues', linewidths=0.5)\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-VGOrEBmTF_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "d6mOCC6uUhO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "-ZE6nA0p-1qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']"
      ],
      "metadata": {
        "id": "5WXuPKoQ-1n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "I1aveQwz-2pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gaussian Naive Bayes model\n",
        "model = GaussianNB()\n"
      ],
      "metadata": {
        "id": "H1uSNmOm-244"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y_train to integers if it's categorical\n",
        "y_train = y_train.astype(int)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "hO-gzSnr-3BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "frIBfz4HP4h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Check unique values and their counts in y_test\n",
        "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "print(\"Unique values in y_test:\", unique_test)\n",
        "print(\"Counts in y_test:\", counts_test)\n",
        "\n",
        "# Check unique values and their counts in y_train\n",
        "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "print(\"\\nUnique values in y_train:\", unique_train)\n",
        "print(\"Counts in y_train:\", counts_train)"
      ],
      "metadata": {
        "id": "qcYwt3ABP4eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classification"
      ],
      "metadata": {
        "id": "Je1o_FojRQSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "# Check data types of y_test and y_pred\n",
        "print(\"Data type of y_test:\", y_test.dtype)\n",
        "print(\"Data type of y_pred:\", y_pred.dtype)\n",
        "\n",
        "# If y_test is continuous, convert it to discrete values using a threshold\n",
        "# Assuming 'y_test' is continuous and needs to be binarized\n",
        "if np.issubdtype(y_test.dtype, np.floating):\n",
        "    # Define your threshold (e.g., 0.5 for binary classification)\n",
        "    threshold = 0.5\n",
        "    binarizer = Binarizer(threshold=threshold)\n",
        "\n",
        "    # Convert y_test to binary discrete values\n",
        "    y_test_binarized = binarizer.fit_transform(y_test.values.reshape(-1, 1)).flatten()\n",
        "    print(\"y_test after binarization:\", y_test_binarized[:10])  # Display the first 10 values to verify\n",
        "\n",
        "    # Ensure y_pred is also in the same format (binarization if necessary)\n",
        "    y_pred_binarized = binarizer.transform(y_pred.reshape(-1, 1)).flatten()\n",
        "    print(\"y_pred after binarization:\", y_pred_binarized[:10])  # Display the first 10 values to verify\n",
        "else:\n",
        "    y_test_binarized = y_test\n",
        "    y_pred_binarized = y_pred\n",
        "\n",
        "# Now, generate the classification report with the binarized labels\n",
        "print('Classification Report:')\n",
        "report = classification_report(y_test_binarized, y_pred_binarized)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "WzUbAQhAP4cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Revised Naive Bayes after Hypertuning parameters"
      ],
      "metadata": {
        "id": "dsBiarCBRlJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = GaussianNB()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "l8vyme-URXDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your data is already loaded into a DataFrame 'data'\n",
        "\n",
        "# Split the data into features and target\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Naive Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Now you can use the code to plot the confusion matrix\n",
        "cm_nb = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "cm_df_nb = pd.DataFrame(cm_nb, index=['Actual Benign', 'Actual Malignant'], columns=['Predicted Benign', 'Predicted Malignant'])\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df_nb, annot=True, fmt='d', cmap='Blues', linewidths=0.5)\n",
        "plt.title('Confusion Matrix for Naive Bayes')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ggzGlswCUJJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K Nearest Neighbours"
      ],
      "metadata": {
        "id": "MuU0GRlETfwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the KNN model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "4SDgtqMxRXeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN after hypertuning parameters"
      ],
      "metadata": {
        "id": "UOIuQhCFT_wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the KNN model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model with the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Get the best model\n",
        "best_knn = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_knn.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "Ef3u3VC7TuCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming best_knn is the trained KNN model and X_test, y_test are defined\n",
        "\n",
        "# Calculate permutation feature importance\n",
        "result = permutation_importance(best_knn, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')\n",
        "\n",
        "# Get feature importance scores\n",
        "importance = result.importances_mean\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': importance\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
        "plt.xlabel('Mean Decrease in Accuracy')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Permutation Feature Importance for KNN')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-W4xYRTDSf0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and plot the confusion matrix\n",
        "cm_knn = confusion_matrix(y_test, y_pred)\n",
        "cm_df_knn = pd.DataFrame(cm_knn, index=['Actual Benign', 'Actual Malignant'], columns=['Predicted Benign', 'Predicted Malignant'])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df_knn, annot=True, fmt='d', cmap='Blues', linewidths=0.5)\n",
        "plt.title('Confusion Matrix for K-Nearest Neighbors (Best Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l1I2OL7oRXcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classifier"
      ],
      "metadata": {
        "id": "_fUms6Z-Vgwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "tiIVnOOORXZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypertuning Random Forest"
      ],
      "metadata": {
        "id": "ED8BC8B6WfVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],        # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],        # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],        # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],          # Minimum number of samples required to be at a leaf node\n",
        "    'bootstrap': [True, False]              # Whether bootstrap samples are used when building trees\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model with the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Get the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "f4jxmMDaRXXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Importance for Random Forest**"
      ],
      "metadata": {
        "id": "kt6NA1RhXkTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming best_rf is the trained RandomForestClassifier model\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances = best_rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance in Random Forest')\n",
        "plt.gca().invert_yaxis()  # Invert the y-axis to have the most important feature on top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jBYImdK7Xk1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the confusion matrix\n",
        "cm_rf = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "cm_df_rf = pd.DataFrame(cm_rf, index=['Actual Benign', 'Actual Malignant'], columns=['Predicted Benign', 'Predicted Malignant'])\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df_rf, annot=True, fmt='d', cmap='Blues', linewidths=0.5)\n",
        "plt.title('Confusion Matrix for Random Forest Classifier (Best Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ocwJ_eNRXVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier"
      ],
      "metadata": {
        "id": "dYORHYRbe1Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "cznxnHEuRXSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree after Hypertuning"
      ],
      "metadata": {
        "id": "3bFs3A6LewDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],          # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],          # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],            # Minimum number of samples required to be at a leaf node\n",
        "    'criterion': ['gini', 'entropy'],         # Function to measure the quality of a split\n",
        "    'max_features': [None, 'sqrt', 'log2'],   # Number of features to consider when looking for the best split\n",
        "    'splitter': ['best', 'random']            # The strategy used to choose the split at each node\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model with the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Get the best model\n",
        "best_dt = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_dt.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "LSAW3y1GRXP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance in Decision Tree Classifier"
      ],
      "metadata": {
        "id": "00jNaZigapEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Assuming you have trained a Decision Tree model named 'dt_model'\n",
        "# Example:\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = dt_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "features_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=features_df)\n",
        "plt.title('Feature Importance in Decision Tree')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eiwph4ALRXNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the confusion matrix\n",
        "cm_dt = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "cm_df_dt = pd.DataFrame(cm_dt, index=['Actual Benign', 'Actual Malignant'], columns=['Predicted Benign', 'Predicted Malignant'])\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df_dt, annot=True, fmt='d', cmap='Blues', linewidths=0.5)\n",
        "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t0WCMtXARXK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "dSQV6NnmgXYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "VMW0RCmRgEy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypertuning Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "UrlNdTzSgpEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Define a smaller parameter grid for faster hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],         # Reduced number of boosting stages\n",
        "    'learning_rate': [0.05, 0.1],       # Two options for learning rate\n",
        "    'max_depth': [3, 4],                # Maximum depth of individual trees\n",
        "    'min_samples_split': [2, 5],        # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2],         # Minimum number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV with a smaller grid\n",
        "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model with the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Get the best model\n",
        "best_gb = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_gb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('Classification Report:')\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "kaIqvbjvgoMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance for Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "uv70LaukbOWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming best_gb is the trained GradientBoostingClassifier model\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances = best_gb.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance in Gradient Boosting')\n",
        "plt.gca().invert_yaxis()  # Invert the y-axis to have the most important feature on top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QwbKenGZbmVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and plot the confusion matrix\n",
        "cm_gb = confusion_matrix(y_test, y_pred)\n",
        "cm_df_gb = pd.DataFrame(cm_gb, index=['Actual Benign', 'Actual Malignant'], columns=['Predicted Benign', 'Predicted Malignant'])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df_gb, annot=True, fmt='d', cmap='Blues', linewidths=0.5)\n",
        "plt.title('Confusion Matrix for Gradient Boosting Classifier (Best Model)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5hQuYcSkgEva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'breast_cancer_data2.csv'  # Replace with your file path if different\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop('diagnosis', axis=1)\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the models\n",
        "models = {\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Logistic Regression': LogisticRegression(solver='liblinear', random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Dictionary to store performance metrics\n",
        "performance = {}\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Perform cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    mean_cv_score = cv_scores.mean()\n",
        "\n",
        "    # Store the results\n",
        "    performance[model_name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1,\n",
        "        'Mean CV Score': mean_cv_score\n",
        "    }\n",
        "\n",
        "# Convert the performance dictionary to a DataFrame for better visualization\n",
        "performance_df = pd.DataFrame(performance).T\n",
        "\n",
        "# Identify the best-performing model\n",
        "best_model = performance_df.idxmax()\n",
        "\n",
        "# Print the performance of each model\n",
        "print(\"Performance Comparison of Models:\\n\")\n",
        "print(performance_df)\n",
        "\n",
        "# Print the best model by each metric\n",
        "print(\"\\nBest Model by Accuracy:\", best_model['Accuracy'])\n",
        "print(\"Best Model by Precision:\", best_model['Precision'])\n",
        "print(\"Best Model by Recall:\", best_model['Recall'])\n",
        "print(\"Best Model by F1 Score:\", best_model['F1 Score'])\n",
        "print(\"Best Model by Mean CV Score:\", best_model['Mean CV Score'])\n"
      ],
      "metadata": {
        "id": "UvduuONzgEtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision recall curve for the models - KNN, Naive Bayes, Logistic Regression, Gradient Boosting, Decision Tree and Random Forest\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ... (Your existing code for model training and evaluation)\n",
        "\n",
        "# Dictionary to store precision-recall curves and AUC scores\n",
        "pr_curves = {}\n",
        "\n",
        "# Calculate precision-recall curves for each model\n",
        "for model_name, model in models.items():\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_scores = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "    else:\n",
        "        y_scores = model.decision_function(X_test)  # Decision function for models like SVM\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    pr_curves[model_name] = {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'auc': pr_auc\n",
        "    }\n",
        "\n",
        "# Plot the precision-recall curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "for model_name, curve in pr_curves.items():\n",
        "    plt.plot(curve['recall'], curve['precision'], label=f'{model_name} (AUC = {curve[\"auc\"]:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curves')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gwc3Sco2gEjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUC curve for the models - KNN, Naive Bayes, Logistic Regression, Gradient Boosting, Decision Tree and Random Forest\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Dictionary to store ROC curves and AUC scores\n",
        "roc_curves = {}\n",
        "\n",
        "# Calculate ROC curves for each model\n",
        "for model_name, model in models.items():\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_scores = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "    else:\n",
        "        y_scores = model.decision_function(X_test)  # Decision function for models like SVM\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    roc_curves[model_name] = {\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'auc': roc_auc\n",
        "    }\n",
        "\n",
        "# Plot the ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "for model_name, curve in roc_curves.items():\n",
        "    plt.plot(curve['fpr'], curve['tpr'], label=f'{model_name} (AUC = {curve[\"auc\"]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random classifier\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OBx-xtoOmYhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IklEeeHumYdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}